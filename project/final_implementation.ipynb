{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing of the data\n",
    "#### First we drop the unused data, or NA values\n",
    "#### Then we convert the genres into numerical values so it can be used by the machine learning algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50005 entries, 0 to 50004\n",
      "Data columns (total 18 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   instance_id       50000 non-null  float64\n",
      " 1   artist_name       50000 non-null  object \n",
      " 2   track_name        50000 non-null  object \n",
      " 3   popularity        50000 non-null  float64\n",
      " 4   acousticness      50000 non-null  float64\n",
      " 5   danceability      50000 non-null  float64\n",
      " 6   duration_ms       50000 non-null  float64\n",
      " 7   energy            50000 non-null  float64\n",
      " 8   instrumentalness  50000 non-null  float64\n",
      " 9   key               50000 non-null  object \n",
      " 10  liveness          50000 non-null  float64\n",
      " 11  loudness          50000 non-null  float64\n",
      " 12  mode              50000 non-null  object \n",
      " 13  speechiness       50000 non-null  float64\n",
      " 14  tempo             50000 non-null  object \n",
      " 15  obtained_date     50000 non-null  object \n",
      " 16  valence           50000 non-null  float64\n",
      " 17  music_genre       50000 non-null  object \n",
      "dtypes: float64(11), object(7)\n",
      "memory usage: 6.9+ MB\n"
     ]
    }
   ],
   "source": [
    "#load data, drop unnecessary columns, and drop all NA values\n",
    "df = pd.read_csv('music_genre.csv')\n",
    "df.info()\n",
    "features = ['instance_id','popularity','acousticness','danceability','duration_ms','energy','instrumentalness','key','liveness','loudness','mode','speechiness','tempo','valence']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above features are proposed to do a first revision on the data, things like Artist name,  title and the obtained data are already left out, since they have no direct link to a specific genre. One could say that an artist would probably stick to a specific genre, but this knowledge is irrelevant to new artists, and would lead to overfitting. To check wether we should drop any other columns, we plot the distributions for all the remaining features for each specific genre using seaborn. In this step, key and mode are already one hot encoded, since they are categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50005 entries, 0 to 50004\n",
      "Data columns (total 18 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   instance_id       50000 non-null  float64\n",
      " 1   artist_name       50000 non-null  object \n",
      " 2   track_name        50000 non-null  object \n",
      " 3   popularity        50000 non-null  float64\n",
      " 4   acousticness      50000 non-null  float64\n",
      " 5   danceability      50000 non-null  float64\n",
      " 6   duration_ms       50000 non-null  float64\n",
      " 7   energy            50000 non-null  float64\n",
      " 8   instrumentalness  50000 non-null  float64\n",
      " 9   key               50000 non-null  object \n",
      " 10  liveness          50000 non-null  float64\n",
      " 11  loudness          50000 non-null  float64\n",
      " 12  mode              50000 non-null  object \n",
      " 13  speechiness       50000 non-null  float64\n",
      " 14  tempo             50000 non-null  object \n",
      " 15  obtained_date     50000 non-null  object \n",
      " 16  valence           50000 non-null  float64\n",
      " 17  music_genre       50000 non-null  object \n",
      "dtypes: float64(11), object(7)\n",
      "memory usage: 6.9+ MB\n"
     ]
    }
   ],
   "source": [
    "#load data, drop unnecessary columns, and drop all NA values\n",
    "df = pd.read_csv('music_genre.csv')\n",
    "df.info()\n",
    "\n",
    "# Drop rows with NaN values across the dataset\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "features = ['instance_id','popularity','acousticness','danceability','duration_ms','energy','instrumentalness','key','liveness','loudness','mode','speechiness','tempo','valence']\n",
    "numeric_features = ['instance_id','popularity','acousticness','danceability','duration_ms','energy','instrumentalness','liveness','loudness','speechiness','valence']\n",
    "categoric_features = ['key', 'mode']\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "mode_encoded = encoder.fit_transform(df[['mode']]).toarray()\n",
    "mode_encoded_df = pd.DataFrame(mode_encoded, columns=encoder.get_feature_names_out(['mode']))\n",
    "df = pd.concat([df, mode_encoded_df], axis=1)\n",
    "df.drop(['mode'], axis=1, inplace=True)\n",
    "\n",
    "key_encoded = encoder.fit_transform(df[['key']]).toarray()\n",
    "key_encoded_df = pd.DataFrame(key_encoded, columns=encoder.get_feature_names_out(['key']))\n",
    "df = pd.concat([df, key_encoded_df], axis=1)\n",
    "df.drop(['key'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'music_genre'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\conda\\envs\\tf\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\conda\\envs\\tf\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\conda\\envs\\tf\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'music_genre'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m numeric_features:\n\u001b[1;32m----> 3\u001b[0m     fig \u001b[38;5;241m=\u001b[39m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFacetGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmusic_genre\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpalette\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSet2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m      4\u001b[0m     fig\u001b[38;5;241m.\u001b[39mmap(sns\u001b[38;5;241m.\u001b[39mkdeplot, feature, fill\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m     fig\u001b[38;5;241m.\u001b[39madd_legend()\n",
      "File \u001b[1;32md:\\conda\\envs\\tf\\lib\\site-packages\\seaborn\\axisgrid.py:383\u001b[0m, in \u001b[0;36mFacetGrid.__init__\u001b[1;34m(self, data, row, col, hue, col_wrap, sharex, sharey, height, aspect, palette, row_order, col_order, hue_order, hue_kws, dropna, legend_out, despine, margin_titles, xlim, ylim, subplot_kws, gridspec_kws)\u001b[0m\n\u001b[0;32m    381\u001b[0m     hue_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 383\u001b[0m     hue_names \u001b[38;5;241m=\u001b[39m categorical_order(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhue\u001b[49m\u001b[43m]\u001b[49m, hue_order)\n\u001b[0;32m    385\u001b[0m colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_palette(data, hue, hue_order, palette)\n\u001b[0;32m    387\u001b[0m \u001b[38;5;66;03m# Set up the lists of names for the row and column facet variables\u001b[39;00m\n",
      "File \u001b[1;32md:\\conda\\envs\\tf\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32md:\\conda\\envs\\tf\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'music_genre'"
     ]
    }
   ],
   "source": [
    "for feature in numeric_features:\n",
    "    \n",
    "    fig = sns.FacetGrid(df, hue=\"music_genre\", aspect=3, palette=\"Set2\") \n",
    "    fig.map(sns.kdeplot, feature, fill=True)\n",
    "    fig.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 49995 entries, 41369 to 34972\n",
      "Data columns (total 26 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   popularity        49995 non-null  float64\n",
      " 1   acousticness      49995 non-null  float64\n",
      " 2   danceability      49995 non-null  float64\n",
      " 3   duration_ms       49995 non-null  float64\n",
      " 4   energy            49995 non-null  float64\n",
      " 5   instrumentalness  49995 non-null  float64\n",
      " 6   liveness          49995 non-null  float64\n",
      " 7   loudness          49995 non-null  float64\n",
      " 8   speechiness       49995 non-null  float64\n",
      " 9   valence           49995 non-null  float64\n",
      " 10  music_genre       49995 non-null  int32  \n",
      " 11  mode_Major        49995 non-null  float64\n",
      " 12  mode_Minor        49995 non-null  float64\n",
      " 13  key_A             49995 non-null  float64\n",
      " 14  key_A#            49995 non-null  float64\n",
      " 15  key_B             49995 non-null  float64\n",
      " 16  key_C             49995 non-null  float64\n",
      " 17  key_C#            49995 non-null  float64\n",
      " 18  key_D             49995 non-null  float64\n",
      " 19  key_D#            49995 non-null  float64\n",
      " 20  key_E             49995 non-null  float64\n",
      " 21  key_F             49995 non-null  float64\n",
      " 22  key_F#            49995 non-null  float64\n",
      " 23  key_G             49995 non-null  float64\n",
      " 24  key_G#            49995 non-null  float64\n",
      " 25  key_nan           49995 non-null  float64\n",
      "dtypes: float64(25), int32(1)\n",
      "memory usage: 10.1 MB\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(['instance_id', 'artist_name', 'track_name','tempo', 'obtained_date'], axis=1) \n",
    "df = df.dropna()\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "genre_encoder  = LabelEncoder()\n",
    "df['music_genre'] = genre_encoder.fit_transform(df['music_genre'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('music_genre', axis=1)\n",
    "Y = df['music_genre']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=777)\n",
    "scaler = StandardScaler()\n",
    "size = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of a Dense layered Neural network for multi class classification from the lab session, but adjusted to get the best solution for our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(10, activation='softmax'), \n",
    "        \n",
    "    ], name=\"Complex\"\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), \n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),  \n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 271.7460 - accuracy: 0.1157 - val_loss: 7.2301 - val_accuracy: 0.1234\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 10.8098 - accuracy: 0.1277 - val_loss: 4.5209 - val_accuracy: 0.1223\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 5.7033 - accuracy: 0.1328 - val_loss: 2.3519 - val_accuracy: 0.1472\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 3.0113 - accuracy: 0.1409 - val_loss: 2.9102 - val_accuracy: 0.1353\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 3.3326 - accuracy: 0.1378 - val_loss: 2.2125 - val_accuracy: 0.1348\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.3175 - accuracy: 0.1370 - val_loss: 2.2054 - val_accuracy: 0.1363\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 4.2123 - accuracy: 0.1377 - val_loss: 2.2093 - val_accuracy: 0.1344\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.2135 - accuracy: 0.1400 - val_loss: 2.2075 - val_accuracy: 0.1358\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.2032 - accuracy: 0.1393 - val_loss: 2.1999 - val_accuracy: 0.1407\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.2024 - accuracy: 0.1394 - val_loss: 2.2034 - val_accuracy: 0.1393\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 6.4632 - accuracy: 0.1336 - val_loss: 2.2066 - val_accuracy: 0.1386\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.2018 - accuracy: 0.1408 - val_loss: 2.1996 - val_accuracy: 0.1436\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.2042 - accuracy: 0.1400 - val_loss: 2.2074 - val_accuracy: 0.1353\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.3106 - accuracy: 0.1394 - val_loss: 2.2068 - val_accuracy: 0.1411\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.2850 - accuracy: 0.1417 - val_loss: 2.1983 - val_accuracy: 0.1403\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.1971 - accuracy: 0.1403 - val_loss: 2.1951 - val_accuracy: 0.1403\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.3761 - accuracy: 0.1437 - val_loss: 2.2030 - val_accuracy: 0.1354\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.2259 - accuracy: 0.1419 - val_loss: 2.1951 - val_accuracy: 0.1441\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1971 - accuracy: 0.1428 - val_loss: 2.2003 - val_accuracy: 0.1421\n",
      "Epoch 20/100\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.2096 - accuracy: 0.1409 - val_loss: 2.2139 - val_accuracy: 0.1327\n",
      "Epoch 21/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1985 - accuracy: 0.1402 - val_loss: 2.2008 - val_accuracy: 0.1380\n",
      "Epoch 22/100\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.1959 - accuracy: 0.1422 - val_loss: 2.1937 - val_accuracy: 0.1458\n",
      "Epoch 23/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.7299 - accuracy: 0.1422 - val_loss: 2.2041 - val_accuracy: 0.1381\n",
      "Epoch 24/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1916 - accuracy: 0.1407 - val_loss: 2.1957 - val_accuracy: 0.1397\n",
      "Epoch 25/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1913 - accuracy: 0.1447 - val_loss: 2.1985 - val_accuracy: 0.1387\n",
      "Epoch 26/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1901 - accuracy: 0.1428 - val_loss: 2.2000 - val_accuracy: 0.1409\n",
      "Epoch 27/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.2322 - accuracy: 0.1433 - val_loss: 2.1976 - val_accuracy: 0.1409\n",
      "Epoch 28/100\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.1886 - accuracy: 0.1434 - val_loss: 2.1960 - val_accuracy: 0.1417\n",
      "Epoch 29/100\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.1866 - accuracy: 0.1453 - val_loss: 2.1924 - val_accuracy: 0.1419\n",
      "Epoch 30/100\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.1854 - accuracy: 0.1448 - val_loss: 2.1970 - val_accuracy: 0.1367\n",
      "Epoch 31/100\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.1848 - accuracy: 0.1433 - val_loss: 2.1997 - val_accuracy: 0.1391\n",
      "Epoch 32/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1851 - accuracy: 0.1453 - val_loss: 2.2058 - val_accuracy: 0.1362\n",
      "Epoch 33/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1844 - accuracy: 0.1453 - val_loss: 2.1929 - val_accuracy: 0.1392\n",
      "Epoch 34/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1830 - accuracy: 0.1448 - val_loss: 2.1965 - val_accuracy: 0.1427\n",
      "Epoch 35/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1815 - accuracy: 0.1465 - val_loss: 2.1936 - val_accuracy: 0.1417\n",
      "Epoch 36/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1809 - accuracy: 0.1451 - val_loss: 2.2033 - val_accuracy: 0.1367\n",
      "Epoch 37/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1817 - accuracy: 0.1464 - val_loss: 2.2031 - val_accuracy: 0.1422\n",
      "Epoch 38/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1797 - accuracy: 0.1479 - val_loss: 2.1955 - val_accuracy: 0.1411\n",
      "Epoch 39/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.2955 - accuracy: 0.1474 - val_loss: 2.2001 - val_accuracy: 0.1405\n",
      "Epoch 40/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1799 - accuracy: 0.1427 - val_loss: 2.1988 - val_accuracy: 0.1374\n",
      "Epoch 41/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 3.2483 - accuracy: 0.1467 - val_loss: 2.2011 - val_accuracy: 0.1367\n",
      "Epoch 42/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1768 - accuracy: 0.1476 - val_loss: 2.2015 - val_accuracy: 0.1386\n",
      "Epoch 43/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1754 - accuracy: 0.1490 - val_loss: 2.2031 - val_accuracy: 0.1422\n",
      "Epoch 44/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1752 - accuracy: 0.1480 - val_loss: 2.2014 - val_accuracy: 0.1423\n",
      "Epoch 45/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1739 - accuracy: 0.1508 - val_loss: 2.2118 - val_accuracy: 0.1437\n",
      "Epoch 46/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 3.9932 - accuracy: 0.1468 - val_loss: 2.2088 - val_accuracy: 0.1414\n",
      "Epoch 47/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 5.2821 - accuracy: 0.1486 - val_loss: 2.2074 - val_accuracy: 0.1429\n",
      "Epoch 48/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1933 - accuracy: 0.1489 - val_loss: 2.2039 - val_accuracy: 0.1398\n",
      "Epoch 49/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.2678 - accuracy: 0.1486 - val_loss: 2.2115 - val_accuracy: 0.1385\n",
      "Epoch 50/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1690 - accuracy: 0.1493 - val_loss: 2.2070 - val_accuracy: 0.1415\n",
      "Epoch 51/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1699 - accuracy: 0.1508 - val_loss: 2.2117 - val_accuracy: 0.1390\n",
      "Epoch 52/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1671 - accuracy: 0.1511 - val_loss: 2.2080 - val_accuracy: 0.1412\n",
      "Epoch 53/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1671 - accuracy: 0.1510 - val_loss: 2.2183 - val_accuracy: 0.1415\n",
      "Epoch 54/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1687 - accuracy: 0.1525 - val_loss: 2.2132 - val_accuracy: 0.1432\n",
      "Epoch 55/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1668 - accuracy: 0.1515 - val_loss: 2.2130 - val_accuracy: 0.1445\n",
      "Epoch 56/100\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.1669 - accuracy: 0.1506 - val_loss: 2.2254 - val_accuracy: 0.1368\n",
      "Epoch 57/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1812 - accuracy: 0.1507 - val_loss: 2.2102 - val_accuracy: 0.1427\n",
      "Epoch 58/100\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.7395 - accuracy: 0.1517 - val_loss: 2.2193 - val_accuracy: 0.1418\n",
      "Epoch 59/100\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.1615 - accuracy: 0.1528 - val_loss: 2.2216 - val_accuracy: 0.1406\n",
      "Epoch 60/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1625 - accuracy: 0.1517 - val_loss: 2.2210 - val_accuracy: 0.1374\n",
      "Epoch 61/100\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.5303 - accuracy: 0.1512 - val_loss: 2.2240 - val_accuracy: 0.1394\n",
      "Epoch 62/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1610 - accuracy: 0.1518 - val_loss: 2.2260 - val_accuracy: 0.1410\n",
      "Epoch 63/100\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.1587 - accuracy: 0.1518 - val_loss: 2.2348 - val_accuracy: 0.1405\n",
      "Epoch 64/100\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.1603 - accuracy: 0.1521 - val_loss: 2.2198 - val_accuracy: 0.1422\n",
      "Epoch 65/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1589 - accuracy: 0.1521 - val_loss: 2.2278 - val_accuracy: 0.1383\n",
      "Epoch 66/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.6147 - accuracy: 0.1524 - val_loss: 2.2307 - val_accuracy: 0.1414\n",
      "Epoch 67/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.3959 - accuracy: 0.1551 - val_loss: 2.2373 - val_accuracy: 0.1432\n",
      "Epoch 68/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1561 - accuracy: 0.1555 - val_loss: 2.2319 - val_accuracy: 0.1381\n",
      "Epoch 69/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1527 - accuracy: 0.1542 - val_loss: 2.2426 - val_accuracy: 0.1435\n",
      "Epoch 70/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1590 - accuracy: 0.1523 - val_loss: 2.2475 - val_accuracy: 0.1356\n",
      "Epoch 71/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1541 - accuracy: 0.1529 - val_loss: 2.2381 - val_accuracy: 0.1385\n",
      "Epoch 72/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1518 - accuracy: 0.1564 - val_loss: 2.2430 - val_accuracy: 0.1392\n",
      "Epoch 73/100\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.1528 - accuracy: 0.1581 - val_loss: 2.2410 - val_accuracy: 0.1376\n",
      "Epoch 74/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.3583 - accuracy: 0.1583 - val_loss: 2.2449 - val_accuracy: 0.1400\n",
      "Epoch 75/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1498 - accuracy: 0.1532 - val_loss: 2.2448 - val_accuracy: 0.1425\n",
      "Epoch 76/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1498 - accuracy: 0.1574 - val_loss: 2.2665 - val_accuracy: 0.1436\n",
      "Epoch 77/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.2128 - accuracy: 0.1548 - val_loss: 2.2590 - val_accuracy: 0.1378\n",
      "Epoch 78/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1495 - accuracy: 0.1567 - val_loss: 2.2642 - val_accuracy: 0.1377\n",
      "Epoch 79/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1462 - accuracy: 0.1571 - val_loss: 2.2558 - val_accuracy: 0.1384\n",
      "Epoch 80/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1489 - accuracy: 0.1580 - val_loss: 2.2610 - val_accuracy: 0.1355\n",
      "Epoch 81/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1479 - accuracy: 0.1586 - val_loss: 2.2651 - val_accuracy: 0.1388\n",
      "Epoch 82/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1606 - accuracy: 0.1586 - val_loss: 2.2708 - val_accuracy: 0.1379\n",
      "Epoch 83/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1464 - accuracy: 0.1587 - val_loss: 2.2939 - val_accuracy: 0.1375\n",
      "Epoch 84/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1469 - accuracy: 0.1614 - val_loss: 2.2616 - val_accuracy: 0.1401\n",
      "Epoch 85/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1440 - accuracy: 0.1594 - val_loss: 2.2709 - val_accuracy: 0.1359\n",
      "Epoch 86/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1436 - accuracy: 0.1585 - val_loss: 2.2760 - val_accuracy: 0.1375\n",
      "Epoch 87/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1472 - accuracy: 0.1594 - val_loss: 2.2710 - val_accuracy: 0.1384\n",
      "Epoch 88/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1663 - accuracy: 0.1583 - val_loss: 2.2714 - val_accuracy: 0.1389\n",
      "Epoch 89/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1407 - accuracy: 0.1589 - val_loss: 2.2914 - val_accuracy: 0.1366\n",
      "Epoch 90/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1448 - accuracy: 0.1572 - val_loss: 2.2771 - val_accuracy: 0.1366\n",
      "Epoch 91/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1431 - accuracy: 0.1588 - val_loss: 2.2819 - val_accuracy: 0.1378\n",
      "Epoch 92/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1388 - accuracy: 0.1599 - val_loss: 2.2859 - val_accuracy: 0.1363\n",
      "Epoch 93/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.2413 - accuracy: 0.1616 - val_loss: 2.2814 - val_accuracy: 0.1367\n",
      "Epoch 94/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1399 - accuracy: 0.1627 - val_loss: 2.2969 - val_accuracy: 0.1354\n",
      "Epoch 95/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1385 - accuracy: 0.1620 - val_loss: 2.3052 - val_accuracy: 0.1363\n",
      "Epoch 96/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1387 - accuracy: 0.1603 - val_loss: 2.3157 - val_accuracy: 0.1348\n",
      "Epoch 97/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 17.4819 - accuracy: 0.1619 - val_loss: 2.3067 - val_accuracy: 0.1418\n",
      "Epoch 98/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1401 - accuracy: 0.1587 - val_loss: 2.2947 - val_accuracy: 0.1372\n",
      "Epoch 99/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1342 - accuracy: 0.1613 - val_loss: 2.3548 - val_accuracy: 0.1347\n",
      "Epoch 100/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.1370 - accuracy: 0.1622 - val_loss: 2.3100 - val_accuracy: 0.1345\n"
     ]
    }
   ],
   "source": [
    "train = model.fit(X_train , Y_train , validation_data=(X_test,Y_test),epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3100 - accuracy: 0.1345\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression can only be done for binary classification, to solve this, we can apply one hot encoding, this way we can use the regression to differ one genre from the others\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50005 entries, 0 to 50004\n",
      "Data columns (total 18 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   instance_id       50000 non-null  float64\n",
      " 1   artist_name       50000 non-null  object \n",
      " 2   track_name        50000 non-null  object \n",
      " 3   popularity        50000 non-null  float64\n",
      " 4   acousticness      50000 non-null  float64\n",
      " 5   danceability      50000 non-null  float64\n",
      " 6   duration_ms       50000 non-null  float64\n",
      " 7   energy            50000 non-null  float64\n",
      " 8   instrumentalness  50000 non-null  float64\n",
      " 9   key               50000 non-null  object \n",
      " 10  liveness          50000 non-null  float64\n",
      " 11  loudness          50000 non-null  float64\n",
      " 12  mode              50000 non-null  object \n",
      " 13  speechiness       50000 non-null  float64\n",
      " 14  tempo             50000 non-null  object \n",
      " 15  obtained_date     50000 non-null  object \n",
      " 16  valence           50000 non-null  float64\n",
      " 17  music_genre       50000 non-null  object \n",
      "dtypes: float64(11), object(7)\n",
      "memory usage: 6.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('music_genre.csv')\n",
    "df.info()\n",
    "df = df.dropna()\n",
    "\n",
    "features = ['popularity','acousticness','danceability','duration_ms','energy','instrumentalness','key','liveness','loudness','mode','speechiness','tempo','valence']\n",
    "numeric_features = ['popularity','acousticness','danceability','duration_ms','energy','instrumentalness','liveness','loudness','speechiness','valence']\n",
    "categoric_features = ['key', 'mode']\n",
    "\n",
    "encoder_mode = OneHotEncoder()\n",
    "encoder_mode.fit(df[['mode']])\n",
    "mode_encoded = encoder_mode.transform(df[['mode']]).toarray()\n",
    "mode_encoded_df = pd.DataFrame(mode_encoded, columns=encoder_mode.get_feature_names_out(['mode']))\n",
    "df = pd.concat([df, mode_encoded_df], axis=1)\n",
    "df.drop(['mode'], axis=1, inplace=True)\n",
    "\n",
    "# Fit and transform 'key'\n",
    "encoder_key = OneHotEncoder()\n",
    "encoder_key.fit(df[['key']])\n",
    "key_encoded = encoder_key.transform(df[['key']]).toarray()\n",
    "key_encoded_df = pd.DataFrame(key_encoded, columns=encoder_key.get_feature_names_out(['key']))\n",
    "df = pd.concat([df, key_encoded_df], axis=1)\n",
    "df.drop(['key'], axis=1, inplace=True)\n",
    "\n",
    "# Fit and transform 'music_genre'\n",
    "encoder_genre = OneHotEncoder()\n",
    "encoder_genre.fit(df[['music_genre']])\n",
    "genre_encoded = encoder_genre.transform(df[['music_genre']]).toarray()\n",
    "genre_encoded_df = pd.DataFrame(genre_encoded, columns=encoder_genre.get_feature_names_out(['music_genre']))\n",
    "df = pd.concat([df, genre_encoded_df], axis=1)\n",
    "df.drop(['music_genre'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 50005 entries, 0 to 10004\n",
      "Data columns (total 36 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   popularity               50000 non-null  float64\n",
      " 1   acousticness             50000 non-null  float64\n",
      " 2   danceability             50000 non-null  float64\n",
      " 3   duration_ms              50000 non-null  float64\n",
      " 4   energy                   50000 non-null  float64\n",
      " 5   instrumentalness         50000 non-null  float64\n",
      " 6   liveness                 50000 non-null  float64\n",
      " 7   loudness                 50000 non-null  float64\n",
      " 8   speechiness              50000 non-null  float64\n",
      " 9   valence                  50000 non-null  float64\n",
      " 10  mode_Major               50000 non-null  float64\n",
      " 11  mode_Minor               50000 non-null  float64\n",
      " 12  key_A                    50005 non-null  float64\n",
      " 13  key_A#                   50005 non-null  float64\n",
      " 14  key_B                    50005 non-null  float64\n",
      " 15  key_C                    50005 non-null  float64\n",
      " 16  key_C#                   50005 non-null  float64\n",
      " 17  key_D                    50005 non-null  float64\n",
      " 18  key_D#                   50005 non-null  float64\n",
      " 19  key_E                    50005 non-null  float64\n",
      " 20  key_F                    50005 non-null  float64\n",
      " 21  key_F#                   50005 non-null  float64\n",
      " 22  key_G                    50005 non-null  float64\n",
      " 23  key_G#                   50005 non-null  float64\n",
      " 24  key_nan                  50005 non-null  float64\n",
      " 25  music_genre_Alternative  50005 non-null  float64\n",
      " 26  music_genre_Anime        50005 non-null  float64\n",
      " 27  music_genre_Blues        50005 non-null  float64\n",
      " 28  music_genre_Classical    50005 non-null  float64\n",
      " 29  music_genre_Country      50005 non-null  float64\n",
      " 30  music_genre_Electronic   50005 non-null  float64\n",
      " 31  music_genre_Hip-Hop      50005 non-null  float64\n",
      " 32  music_genre_Jazz         50005 non-null  float64\n",
      " 33  music_genre_Rap          50005 non-null  float64\n",
      " 34  music_genre_Rock         50005 non-null  float64\n",
      " 35  music_genre_nan          50005 non-null  float64\n",
      "dtypes: float64(36)\n",
      "memory usage: 14.1 MB\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(['instance_id', 'artist_name', 'track_name','obtained_date','tempo'], axis=1) \n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoded_features = ['popularity', 'acousticness', 'danceability', 'duration_ms', 'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness',  'valence', 'mode_Major', 'mode_Minor', 'key_A', 'key_A#', 'key_B', 'key_C', 'key_C#', 'key_D', 'key_D#', 'key_E', 'key_F', 'key_F#', 'key_G', 'key_G#', 'key_nan']\n",
    "encoded_genres = ['music_genre_Alternative', 'music_genre_Anime', 'music_genre_Blues', 'music_genre_Classical', 'music_genre_Country', 'music_genre_Electronic', 'music_genre_Hip-Hop', 'music_genre_Jazz', 'music_genre_Rap', 'music_genre_Rock']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the following sections, the implementation of the logistic regression from the lab is implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): A scalar, numpy array of any size.\n",
    "\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), with the same shape as z\n",
    "         \n",
    "    \"\"\"\n",
    "          \n",
    "    ### START CODE HERE ### \n",
    "    g = 1/(1+np.exp(-z))\n",
    "    ### END SOLUTION ###  \n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also be interesting to discuss the compute cost for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED FUNCTION: compute_cost\n",
    "def compute_cost(X, y, w, b, lambda_= 1):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (array_like Shape (m,)) target value \n",
    "      w : (array_like Shape (n,)) Values of parameters of the model      \n",
    "      b : scalar Values of bias parameter of the model\n",
    "      lambda_: unused placeholder\n",
    "    Returns:\n",
    "      total_cost: (scalar)         cost \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    zwb = w*X\n",
    "    zwb = np.sum(zwb,axis=1)\n",
    "    zwb += b\n",
    "    fwb = sigmoid(zwb)\n",
    "    cost = (-y*np.log(fwb))-(1-y)*np.log(1-fwb)\n",
    "    total_cost= np.sum(cost)/m\n",
    "    ### END CODE HERE ### \n",
    "\n",
    "    return total_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3\n",
    "# GRADED FUNCTION: compute_gradient\n",
    "def compute_gradient(X, y, w, b, lambda_=None): \n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression \n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) variable such as house size \n",
    "      y : (array_like Shape (m,1)) actual value \n",
    "      w : (array_like Shape (n,1)) values of parameters of the model      \n",
    "      b : (scalar)                 value of parameter of the model \n",
    "      lambda_: unused placeholder.\n",
    "    Returns\n",
    "      dj_dw: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(w.shape)\n",
    "    dj_db = 0.\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    zwb = w*X\n",
    "    zwb = np.sum(zwb,axis=1)\n",
    "    zwb += b\n",
    "    fwb = sigmoid(zwb)\n",
    "    dj_db = (np.sum(fwb-y))/m\n",
    "    dj_dw = (np.dot((fwb-y), X))/m\n",
    "    \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X :    (array_like Shape (m, n)\n",
    "      y :    (array_like Shape (m,))\n",
    "      w_in : (array_like Shape (n,))  Initial values of parameters of the model\n",
    "      b_in : (scalar)                 Initial value of parameter of the model\n",
    "      cost_function:                  function to compute cost\n",
    "      alpha : (float)                 Learning rate\n",
    "      num_iters : (int)               number of iterations to run gradient descent\n",
    "      lambda_ (scalar, float)         regularization constant\n",
    "      \n",
    "    Returns:\n",
    "      w : (array_like Shape (n,)) Updated values of parameters of the model after\n",
    "          running gradient descent\n",
    "      b : (scalar)                Updated value of parameter of the model after\n",
    "          running gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of training examples\n",
    "    m = len(X)\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w_in = w_in - alpha * dj_dw               \n",
    "        b_in = b_in - alpha * dj_db              \n",
    "       \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost =  cost_function(X, y, w_in, b_in, lambda_)\n",
    "            J_history.append(cost)\n",
    "\n",
    "       \n",
    "    return w_in, b_in, J_history, w_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599453\n",
      "Iteration    0: Cost     0.69   \n",
      "Iteration 1000: Cost     0.56   \n",
      "Iteration 2000: Cost     0.48   \n",
      "Iteration 3000: Cost     0.43   \n",
      "Iteration 4000: Cost     0.40   \n",
      "Iteration 5000: Cost     0.38   \n",
      "Iteration 6000: Cost     0.36   \n",
      "Iteration 7000: Cost     0.35   \n",
      "Iteration 8000: Cost     0.35   \n",
      "Iteration 9000: Cost     0.34   \n",
      "Iteration 9999: Cost     0.34   \n"
     ]
    }
   ],
   "source": [
    "X = df[encoded_features]\n",
    "Y = df[encoded_genres[5]]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=777)\n",
    "m, n = X_train.shape\n",
    "# Compute and display cost with w initialized to zeroes\n",
    "initial_w = np.zeros(n)\n",
    "initial_b = 0.\n",
    "cost = compute_cost(X_train, Y_train, initial_w, initial_b)\n",
    "print(cost)\n",
    "\n",
    "\n",
    "\n",
    "# Some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 0.001\n",
    "\n",
    "w,b, J_history,_ = gradient_descent(X_train ,Y_train, initial_w, initial_b, \n",
    "                                   compute_cost, compute_gradient, alpha, iterations, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4\n",
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(X, w, b): \n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic\n",
    "    regression parameters w\n",
    "    \n",
    "    Args:\n",
    "    X : (ndarray Shape (m, n))\n",
    "    w : (array_like Shape (n,))      Parameters of the model\n",
    "    b : (scalar, float)              Parameter of the model\n",
    "\n",
    "    Returns:\n",
    "    p: (ndarray (m,1))\n",
    "        The predictions for X using a threshold at 0.5\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m, n = X.shape   \n",
    "    p = np.zeros(m)\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    for i in range(m):   \n",
    "         z_wb = 0\n",
    "         for j in range(n): \n",
    "             #pandas doesn't support indexing for columns, so .iloc is needed\n",
    "             z_wb_ij = X.iloc[i, j] * w[j]\n",
    "             z_wb += z_wb_ij\n",
    "      \n",
    "         z_wb += b\n",
    "         f_wb = sigmoid(z_wb)\n",
    "         p[i] = f_wb >= 0.5\n",
    "        \n",
    "    ### END CODE HERE ### \n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this is first tested for rock songs, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy for Rock: 90.023498\n"
     ]
    }
   ],
   "source": [
    "p = predict(X_train, w,b)\n",
    "print('Train Accuracy for Rock: %f'%(np.mean(p == Y_train) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jens7\\AppData\\Local\\Temp\\ipykernel_13772\\3686182136.py:23: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = (-y*np.log(fwb))-(1-y)*np.log(1-fwb)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy for music_genre_Alternative89.768523\n",
      "Test Accuracy for music_genre_Alternative89.991001\n",
      "Train Accuracy for music_genre_Anime89.913509\n",
      "Test Accuracy for music_genre_Anime89.521048\n",
      "Train Accuracy for music_genre_Blues89.861014\n",
      "Test Accuracy for music_genre_Blues89.691031\n",
      "Train Accuracy for music_genre_Classical89.831017\n",
      "Test Accuracy for music_genre_Classical89.911009\n",
      "Train Accuracy for music_genre_Country86.691331\n",
      "Test Accuracy for music_genre_Country87.111289\n",
      "Train Accuracy for music_genre_Electronic88.033697\n",
      "Test Accuracy for music_genre_Electronic87.681232\n",
      "Train Accuracy for music_genre_Hip-Hop82.209279\n",
      "Test Accuracy for music_genre_Hip-Hop81.571843\n",
      "Train Accuracy for music_genre_Jazz89.886011\n",
      "Test Accuracy for music_genre_Jazz90.300970\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m initial_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Run gradient descent to learn the weights and bias term\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m w, b, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_cost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_gradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m p \u001b[38;5;241m=\u001b[39m predict(X_train, w,b)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Accuracy for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m genre\u001b[38;5;241m+\u001b[39m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(p \u001b[38;5;241m==\u001b[39m Y_train) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m))\n",
      "Cell \u001b[1;32mIn[37], line 33\u001b[0m, in \u001b[0;36mgradient_descent\u001b[1;34m(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_)\u001b[0m\n\u001b[0;32m     28\u001b[0m w_history \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iters):\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Calculate the gradient and update the parameters\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     dj_db, dj_dw \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# Update Parameters using w, b, alpha and gradient\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     w_in \u001b[38;5;241m=\u001b[39m w_in \u001b[38;5;241m-\u001b[39m alpha \u001b[38;5;241m*\u001b[39m dj_dw               \n",
      "Cell \u001b[1;32mIn[36], line 23\u001b[0m, in \u001b[0;36mcompute_gradient\u001b[1;34m(X, y, w, b, lambda_)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m### START CODE HERE ### \u001b[39;00m\n\u001b[0;32m     22\u001b[0m zwb \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m*\u001b[39mX\n\u001b[1;32m---> 23\u001b[0m zwb \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzwb\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m zwb \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m b\n\u001b[0;32m     25\u001b[0m fwb \u001b[38;5;241m=\u001b[39m sigmoid(zwb)\n",
      "File \u001b[1;32md:\\conda\\envs\\tf\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2314\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\conda\\envs\\tf\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "File \u001b[1;32md:\\conda\\envs\\tf\\lib\\site-packages\\pandas\\core\\generic.py:11797\u001b[0m, in \u001b[0;36mNDFrame._add_numeric_operations.<locals>.sum\u001b[1;34m(self, axis, skipna, level, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[0;32m  11777\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[0;32m  11778\u001b[0m     _num_doc,\n\u001b[0;32m  11779\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn the sum of the values over the requested axis.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11795\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  11796\u001b[0m ):\n\u001b[1;32m> 11797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NDFrame\u001b[38;5;241m.\u001b[39msum(\n\u001b[0;32m  11798\u001b[0m         \u001b[38;5;28mself\u001b[39m, axis, skipna, level, numeric_only, min_count, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m  11799\u001b[0m     )\n",
      "File \u001b[1;32md:\\conda\\envs\\tf\\lib\\site-packages\\pandas\\core\\generic.py:11501\u001b[0m, in \u001b[0;36mNDFrame.sum\u001b[1;34m(self, axis, skipna, level, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[0;32m  11492\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(\n\u001b[0;32m  11493\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  11494\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11499\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  11500\u001b[0m ):\n\u001b[1;32m> 11501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_min_count_stat_function(\n\u001b[0;32m  11502\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m, nanops\u001b[38;5;241m.\u001b[39mnansum, axis, skipna, level, numeric_only, min_count, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m  11503\u001b[0m     )\n",
      "File \u001b[1;32md:\\conda\\envs\\tf\\lib\\site-packages\\pandas\\core\\generic.py:11483\u001b[0m, in \u001b[0;36mNDFrame._min_count_stat_function\u001b[1;34m(self, name, func, axis, skipna, level, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[0;32m  11467\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m  11468\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the level keyword in DataFrame and Series aggregations is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m  11469\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will be removed in a future version. Use groupby \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11472\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m  11473\u001b[0m     )\n\u001b[0;32m  11474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_by_level(\n\u001b[0;32m  11475\u001b[0m         name,\n\u001b[0;32m  11476\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11480\u001b[0m         numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only,\n\u001b[0;32m  11481\u001b[0m     )\n\u001b[1;32m> 11483\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  11484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  11485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  11486\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  11487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  11488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  11489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  11490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\conda\\envs\\tf\\lib\\site-packages\\pandas\\core\\frame.py:10879\u001b[0m, in \u001b[0;36mDataFrame._reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m  10876\u001b[0m values \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m  10878\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m> 10879\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10881\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m  10882\u001b[0m     \u001b[38;5;66;03m# e.g. in nanops trying to convert strs to float\u001b[39;00m\n\u001b[0;32m  10884\u001b[0m     data \u001b[38;5;241m=\u001b[39m _get_data()\n",
      "File \u001b[1;32md:\\conda\\envs\\tf\\lib\\site-packages\\pandas\\core\\frame.py:10817\u001b[0m, in \u001b[0;36mDataFrame._reduce.<locals>.func\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m  10815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(values: np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m  10816\u001b[0m     \u001b[38;5;66;03m# We only use this in the case that operates on self.values\u001b[39;00m\n\u001b[1;32m> 10817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32md:\\conda\\envs\\tf\\lib\\site-packages\\pandas\\core\\nanops.py:93\u001b[0m, in \u001b[0;36mdisallow.__call__.<locals>._f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 93\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# we want to transform an object array\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# ValueError message to the more typical TypeError\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# e.g. this is normally a disallowed function on\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;66;03m# object arrays that contain strings\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(args[\u001b[38;5;241m0\u001b[39m]):\n",
      "File \u001b[1;32md:\\conda\\envs\\tf\\lib\\site-packages\\pandas\\core\\nanops.py:418\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[1;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    416\u001b[0m     mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[1;32m--> 418\u001b[0m result \u001b[38;5;241m=\u001b[39m func(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, mask\u001b[38;5;241m=\u001b[39mmask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n\u001b[0;32m    421\u001b[0m     result \u001b[38;5;241m=\u001b[39m _wrap_results(result, orig_values\u001b[38;5;241m.\u001b[39mdtype, fill_value\u001b[38;5;241m=\u001b[39miNaT)\n",
      "File \u001b[1;32md:\\conda\\envs\\tf\\lib\\site-packages\\pandas\\core\\nanops.py:491\u001b[0m, in \u001b[0;36mmaybe_operate_rowwise.<locals>.newfunc\u001b[1;34m(values, axis, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         results \u001b[38;5;241m=\u001b[39m [func(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrs]\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(results)\n\u001b[1;32m--> 491\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(values, axis\u001b[38;5;241m=\u001b[39maxis, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\conda\\envs\\tf\\lib\\site-packages\\pandas\\core\\nanops.py:622\u001b[0m, in \u001b[0;36mnansum\u001b[1;34m(values, axis, skipna, min_count, mask)\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;129m@disallow\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    589\u001b[0m \u001b[38;5;129m@_datetimelike_compat\u001b[39m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;129m@maybe_operate_rowwise\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    597\u001b[0m     mask: npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mbool_] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    598\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m    599\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;124;03m    Sum the elements along an axis ignoring NaNs\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;124;03m    3.0\u001b[39;00m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 622\u001b[0m     values, mask, dtype, dtype_max, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_get_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    625\u001b[0m     dtype_sum \u001b[38;5;241m=\u001b[39m dtype_max\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_float_dtype(dtype):\n",
      "File \u001b[1;32md:\\conda\\envs\\tf\\lib\\site-packages\\pandas\\core\\nanops.py:332\u001b[0m, in \u001b[0;36m_get_values\u001b[1;34m(values, skipna, fill_value, fill_value_typ, mask)\u001b[0m\n\u001b[0;32m    327\u001b[0m fill_value \u001b[38;5;241m=\u001b[39m _get_fill_value(\n\u001b[0;32m    328\u001b[0m     dtype, fill_value\u001b[38;5;241m=\u001b[39mfill_value, fill_value_typ\u001b[38;5;241m=\u001b[39mfill_value_typ\n\u001b[0;32m    329\u001b[0m )\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skipna \u001b[38;5;129;01mand\u001b[39;00m (mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m dtype_ok \u001b[38;5;129;01mor\u001b[39;00m datetimelike:\n\u001b[0;32m    334\u001b[0m             values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32md:\\conda\\envs\\tf\\lib\\site-packages\\numpy\\core\\_methods.py:58\u001b[0m, in \u001b[0;36m_any\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_any\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_any\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_any(a, axis, dtype, out, keepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterations = 3000\n",
    "\n",
    "# For each genre\n",
    "for genre in encoded_genres:\n",
    "    # Set the target variable Y to be 1 for the current genre and 0 for all other genres\n",
    "    Y = (df[genre] == 1).astype(int)\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=777)\n",
    "\n",
    "    # Initialize the weights and bias term\n",
    "    m, n = X_train.shape\n",
    "    initial_w = np.zeros(n)\n",
    "    initial_b = 0.\n",
    "\n",
    "    # Run gradient descent to learn the weights and bias term\n",
    "    w, b, _, _ = gradient_descent(X_train, Y_train, initial_w, initial_b, compute_cost, compute_gradient, alpha, iterations, 0)\n",
    "    p = predict(X_train, w,b)\n",
    "    print('Train Accuracy for '+ genre+  '%f'%(np.mean(p == Y_train) * 100))\n",
    "    p = predict(X_test, w,b)\n",
    "    print('Test Accuracy for ' + genre +'%f'%(np.mean(p == Y_test) * 100))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the one vs all approach, an accuracy of 90% can be obtained, for each individual genre, however, this doesn't compare multiple genres to each other. Below, the results for the built in one vs all code is also showed. The results are similar, but for the genre Rock, there is less accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: music_genre_Alternative    89.951005\n",
      "music_genre_Anime          89.153585\n",
      "music_genre_Blues          90.035996\n",
      "music_genre_Classical      90.943406\n",
      "music_genre_Country        89.901010\n",
      "music_genre_Electronic     89.998500\n",
      "music_genre_Hip-Hop        82.209279\n",
      "music_genre_Jazz           89.916008\n",
      "music_genre_Rap            89.143586\n",
      "music_genre_Rock           82.206779\n",
      "dtype: float64\n",
      "Test Accuracy: music_genre_Alternative    90.190981\n",
      "music_genre_Anime          88.731127\n",
      "music_genre_Blues          89.821018\n",
      "music_genre_Classical      90.970903\n",
      "music_genre_Country        90.400960\n",
      "music_genre_Electronic     89.901010\n",
      "music_genre_Hip-Hop        81.561844\n",
      "music_genre_Jazz           90.340966\n",
      "music_genre_Rap            88.771123\n",
      "music_genre_Rock           81.931807\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\tf\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3502: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "d:\\conda\\envs\\tf\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3502: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize the one-vs-all classifier with logistic regression as the base classifier\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "X = df[encoded_features].fillna(df[encoded_features].mean(axis=0))\n",
    "Y = df[encoded_genres].fillna(df[encoded_genres].mean(axis=0))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=777)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the classes for the training and test sets\n",
    "p_train = clf.predict(X_train)\n",
    "p_test = clf.predict(X_test)\n",
    "\n",
    "# Compute the train and test accuracies\n",
    "train_accuracy = np.mean(p_train == Y_train) * 100\n",
    "test_accuracy = np.mean(p_test == Y_test) * 100\n",
    "\n",
    "print(f'Train Accuracy: {train_accuracy}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the random forest, the data gets loaded again like in the neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50005 entries, 0 to 50004\n",
      "Data columns (total 18 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   instance_id       50000 non-null  float64\n",
      " 1   artist_name       50000 non-null  object \n",
      " 2   track_name        50000 non-null  object \n",
      " 3   popularity        50000 non-null  float64\n",
      " 4   acousticness      50000 non-null  float64\n",
      " 5   danceability      50000 non-null  float64\n",
      " 6   duration_ms       50000 non-null  float64\n",
      " 7   energy            50000 non-null  float64\n",
      " 8   instrumentalness  50000 non-null  float64\n",
      " 9   key               50000 non-null  object \n",
      " 10  liveness          50000 non-null  float64\n",
      " 11  loudness          50000 non-null  float64\n",
      " 12  mode              50000 non-null  object \n",
      " 13  speechiness       50000 non-null  float64\n",
      " 14  tempo             50000 non-null  object \n",
      " 15  obtained_date     50000 non-null  object \n",
      " 16  valence           50000 non-null  float64\n",
      " 17  music_genre       50000 non-null  object \n",
      "dtypes: float64(11), object(7)\n",
      "memory usage: 6.9+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50005 entries, 0 to 50004\n",
      "Data columns (total 18 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   instance_id       50000 non-null  float64\n",
      " 1   artist_name       50000 non-null  object \n",
      " 2   track_name        50000 non-null  object \n",
      " 3   popularity        50000 non-null  float64\n",
      " 4   acousticness      50000 non-null  float64\n",
      " 5   danceability      50000 non-null  float64\n",
      " 6   duration_ms       50000 non-null  float64\n",
      " 7   energy            50000 non-null  float64\n",
      " 8   instrumentalness  50000 non-null  float64\n",
      " 9   key               50000 non-null  object \n",
      " 10  liveness          50000 non-null  float64\n",
      " 11  loudness          50000 non-null  float64\n",
      " 12  mode              50000 non-null  object \n",
      " 13  speechiness       50000 non-null  float64\n",
      " 14  tempo             50000 non-null  object \n",
      " 15  obtained_date     50000 non-null  object \n",
      " 16  valence           50000 non-null  float64\n",
      " 17  music_genre       50000 non-null  object \n",
      "dtypes: float64(11), object(7)\n",
      "memory usage: 6.9+ MB\n"
     ]
    }
   ],
   "source": [
    "#load data, drop unnecessary columns, and drop all NA values\n",
    "df = pd.read_csv('music_genre.csv')\n",
    "df.info()\n",
    "features = ['instance_id','popularity','acousticness','danceability','duration_ms','energy','instrumentalness','key','liveness','loudness','mode','speechiness','tempo','valence']\n",
    "#load data, drop unnecessary columns, and drop all NA values\n",
    "df = pd.read_csv('music_genre.csv')\n",
    "df.info()\n",
    "\n",
    "# Drop rows with NaN values across the dataset\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "features = ['instance_id','popularity','acousticness','danceability','duration_ms','energy','instrumentalness','key','liveness','loudness','mode','speechiness','tempo','valence']\n",
    "numeric_features = ['instance_id','popularity','acousticness','danceability','duration_ms','energy','instrumentalness','liveness','loudness','speechiness','valence']\n",
    "categoric_features = ['key', 'mode']\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "mode_encoded = encoder.fit_transform(df[['mode']]).toarray()\n",
    "mode_encoded_df = pd.DataFrame(mode_encoded, columns=encoder.get_feature_names_out(['mode']))\n",
    "df = pd.concat([df, mode_encoded_df], axis=1)\n",
    "df.drop(['mode'], axis=1, inplace=True)\n",
    "\n",
    "key_encoded = encoder.fit_transform(df[['key']]).toarray()\n",
    "key_encoded_df = pd.DataFrame(key_encoded, columns=encoder.get_feature_names_out(['key']))\n",
    "df = pd.concat([df, key_encoded_df], axis=1)\n",
    "df.drop(['key'], axis=1, inplace=True)\n",
    "df = df.drop(['instance_id', 'artist_name', 'track_name','tempo', 'obtained_date'], axis=1) \n",
    "df = df.dropna()\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "X = df.drop('music_genre', axis=1)\n",
    "Y = df['music_genre']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 55.56555655565557\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Alternative       0.47      0.36      0.41      1035\n",
      "       Anime       0.79      0.75      0.77       978\n",
      "       Blues       0.60      0.55      0.57       981\n",
      "   Classical       0.82      0.84      0.83       968\n",
      "     Country       0.59      0.59      0.59      1006\n",
      "  Electronic       0.62      0.61      0.62       999\n",
      "     Hip-Hop       0.36      0.41      0.38      1014\n",
      "        Jazz       0.53      0.50      0.51      1003\n",
      "         Rap       0.35      0.32      0.33      1014\n",
      "        Rock       0.48      0.66      0.56      1001\n",
      "\n",
      "    accuracy                           0.56      9999\n",
      "   macro avg       0.56      0.56      0.56      9999\n",
      "weighted avg       0.56      0.56      0.55      9999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#1000 estimators because we have a big dataset\n",
    "randomForest = RandomForestClassifier(n_estimators=1000)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=77)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "randomForest.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the classes for the test set\n",
    "Y_pred = randomForest.predict(X_test)\n",
    "# Compute the test accuracy\n",
    "test_accuracy = accuracy_score(Y_test, Y_pred) * 100\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "print(classification_report(Y_test, Y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a total accuracy of 55.5% This is already way better than the neural network can provide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best cross-validation score: 0.5698568758594825\n",
      "Accuracy: 0.5589558955895589\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Alternative       0.51      0.26      0.34      1035\n",
      "       Anime       0.77      0.73      0.75       978\n",
      "       Blues       0.59      0.50      0.55       981\n",
      "   Classical       0.82      0.83      0.82       968\n",
      "     Country       0.56      0.59      0.57      1006\n",
      "  Electronic       0.59      0.60      0.60       999\n",
      "     Hip-Hop       0.41      0.50      0.45      1014\n",
      "        Jazz       0.52      0.47      0.49      1003\n",
      "         Rap       0.42      0.34      0.37      1014\n",
      "        Rock       0.47      0.80      0.59      1001\n",
      "\n",
      "    accuracy                           0.56      9999\n",
      "   macro avg       0.57      0.56      0.55      9999\n",
      "weighted avg       0.56      0.56      0.55      9999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a set of parameters to test\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model on the training data\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "\n",
    "# Predict and evaluate the model using the best found parameters\n",
    "best_predictions = grid_search.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(Y_test, best_predictions))\n",
    "print(classification_report(Y_test, best_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearch has a comparable result, however, we can see that the precision is more evened out, the bad scores are less bad, and the very good results are less good. So it is more evened out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the XGBoost classifier with some default parameters\n",
    "xgb_classifier = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',  # For multiclass classification\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Encode string class labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "Y_train_encoded = label_encoder.fit_transform(Y_train)\n",
    "Y_test_encoded = label_encoder.transform(Y_test)\n",
    "\n",
    "# Update your pipeline\n",
    "\n",
    "# Fit the model on the encoded training data\n",
    "xgb_classifier.fit(X_train, Y_train_encoded)\n",
    "\n",
    "# Predict and evaluate the model on the encoded test data\n",
    "predictions_encoded = pipeline.predict(X_test)\n",
    "predictions = label_encoder.inverse_transform(predictions_encoded)  # Decode the predictions back to original labels\n",
    "\n",
    "# Predict and evaluate the model\n",
    "predictions = pipeline.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(Y_test_encoded, predictions_encoded))\n",
    "print(classification_report(Y_test_encoded, predictions_encoded, target_names=label_encoder.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
